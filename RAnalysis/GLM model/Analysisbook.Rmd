---
title: "draft"
output:
  pdf_document: default
  html_notebook: default
---
```{r echo = T, results = 'hide'}

library(VineCopula)
library(Ryacas)
library(tinytex)
library(corrplot)
library(caret)

library(RANN)

library(readxl)
library(actuar)
library(fitdistrplus)
library(evir)
library(extRemes)
library(tidyverse)

library(vars)

library(tidyverse)
library(wooldridge)
library(whitestrap)
library(car)
library(olsrr)
source("Customfunctions.R")
library(missForest)
library(VGAM)
library(broom)
library(ggplot2)
library(pROC)
library(statmod)
library(missMDA)
library(recipes)
library(tidymodels)
library(Metrics)
library(POT)

```

# Exploratory analysis
```{r echo = T, results = 'hide'}
data <- read_csv("Car_Claims.csv")

# converting character covariates into factors
for (i in 1:length(data)){
  if (is.character(data[[i]])){
    data[[i]] <- as.factor(data[[i]])
  }
}
summary(data)
```

## splitting data into train and test/validation 
```{r}
set.seed(67)
train_indices <- sample(1:10000,8000)
train_data <- data[train_indices, ]
test_data <- data[-train_indices, ]


train_data_x <- train_data %>% dplyr::select(-OUTCOME, -CLAIMS)
test_data_x <- test_data %>% dplyr::select(-OUTCOME, -CLAIMS)
```

## correlation matrix of numeric covariates
```{r}
num_cols <-  data %>% dplyr::select(where(is.numeric))

num_cols <-  num_cols[!is.na(num_cols$CREDIT_SCORE) & 
                      !is.na(num_cols$ANNUAL_MILEAGE),]
num_cols <-  num_cols %>% select(-ID)
corrplot(cor(num_cols), type = "lower", diag = F)
```
- Annual mileage is lower for married ppl and ppl with children
- past accidents has high corr with speeding violations ofc
- vehicle owners are less likely to file claims?


## investigating missing values
```{r}
print(paste("number of rows with missing annual mileage", 
            sum(is.na(train_data$ANNUAL_MILEAGE)), "/8000"))
print(paste("number of rows with missing creditscore", 
            sum(is.na(train_data$CREDIT_SCORE)), "/8000"))
print(paste("number of rows missing both annual mileage and credit", 
            sum(is.na(train_data$ANNUAL_MILEAGE) & is.na(train_data$CREDIT_SCORE)), "/8000"))

print(paste("percentage of ppl of claim for nonempty annual mileage rows", 
            mean(train_data[!is.na(train_data$ANNUAL_MILEAGE),"OUTCOME", drop = T])))
print(paste("percentage of ppl of claim for empty annual mileage rows", 
            mean(train_data[is.na(train_data$ANNUAL_MILEAGE),"OUTCOME", drop = T])))

print(paste("percentage of ppl of claim for nonempty creditscore rows", 
            mean(train_data[!is.na(train_data$CREDIT_SCORE),"OUTCOME", drop = T])))
print(paste("percentage of ppl of claim for empty creditscore rows", 
            mean(train_data[is.na(train_data$CREDIT_SCORE),"OUTCOME", drop = T])))


boxplot(log(train_data[train_data$CLAIMS>0,"CLAIMS", drop = T]) ~ is.na(train_data[train_data$CLAIMS>0,"ANNUAL_MILEAGE", drop = T]),
        xlab = "is annual mileage missing?", ylab = "log(claim size)")

boxplot(log(train_data[train_data$CLAIMS>0,"CLAIMS", drop = T]) ~ is.na(train_data[train_data$CLAIMS>0,"CREDIT_SCORE", drop = T]),
        xlab = "is creditscore missing?", ylab = "log(claim size)")

```

- Dont want to get rid of ~10% of the data, would be good to impute

Investigating if emptiness of the creditscore is independent to other variables i.e if creditscore is intentionally left empty
- would expect younger ppl to have no credit score, so cells may be intentionally empty
```{r}
print("TESTING FOR INDEPENDENCE OF CREDIT_SCORE EMPTINESS")
# Define the target variable: a binary indicator for missing credit scores
credit_score_missing <- is.na(train_data$CREDIT_SCORE)

# Get a list of all predictor variables to test, excluding ID and CREDIT_SCORE itself
predictors_to_test <- setdiff(names(train_data), c("ID", "CREDIT_SCORE"))

# Loop through each predictor and perform the appropriate test
for (var in predictors_to_test) {
  
  # Ignore any columns with no variation
  if (length(unique(train_data[[var]])) < 2) next
  
  # --- Test for association with CONTINUOUS variables ---
  if (is.numeric(train_data[[var]])) {
    # We use a t-test to see if the mean of the numeric variable is
    # different between the 'missing' and 'present' groups.
    test_result <- t.test(train_data[[var]] ~ credit_score_missing)
    p_value <- test_result$p.value
    cat(sprintf("Variable: %-20s | Test: T-test         | P-value: %.4f\n", var, p_value))
    
  # --- Test for association with CATEGORICAL variables ---
  } else if (is.factor(train_data[[var]]) || is.character(train_data[[var]])) {
    # We use a Chi-squared test for independence between two categorical variables.
    # We add 'simulate.p.value = TRUE' to handle cases with low expected counts.
    test_result <- chisq.test(table(train_data[[var]], credit_score_missing), simulate.p.value = TRUE)
    p_value <- test_result$p.value
    cat(sprintf("Variable: %-20s | Test: Chi-squared    | P-value: %.4f\n", var, p_value))
  }
}
```
-  pvalues very high, so likely that annual_mileage is left empty independently of other variables for all except vehicle year

```{r}

plot_data <- train_data %>%
  # Create a clear factor for whether the credit score is missing
  mutate(Credit_Score_Status = factor(ifelse(is.na(CREDIT_SCORE), "Empty", "Not Empty"))) %>%
  # Count the occurrences of each vehicle year for each status
  count(VEHICLE_YEAR, Credit_Score_Status) %>%
  # Group by the status so we can calculate proportions within each group
  group_by(Credit_Score_Status) %>%
  # Calculate the proportion
  mutate(Proportion = n / sum(n))


# --- 4. Create the Side-by-Side Bar Plot ---

ggplot(plot_data, aes(x = VEHICLE_YEAR, y = Proportion, fill = Credit_Score_Status)) +
  # geom_bar with stat="identity" uses the y-value directly.
  # position="dodge" places the bars next to each other.
  geom_bar(stat = "identity", position = "dodge") +
  
  labs(
    title = "Vehicle Year Distribution by Credit Score Availability",
    subtitle = "Comparing proportions for policies with empty vs. non-empty credit scores",
    x = "Vehicle Year",
    y = "Proportion within Group",
    fill = "Credit Score Status"
  ) +
  
  # Set the colors as requested
  scale_fill_manual(values = c("Empty" = "gold", "Not Empty" = "dodgerblue")) +
  
  # Improve readability
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```
- with no particular reason to take the conventional significance level of 0.05, conclude that the vehicle year is still independent to the emptiness of credit scores

- suggests that ppl with no credit score are not necessarily younger, so we assume that missing values are missing unintentionally


```{r}
print("TESTING FOR INDEPENDENCE OF ANNUAL_MILEAGE EMPTINESS")

AM_missing <- is.na(train_data$ANNUAL_MILEAGE)

# Get a list of all predictor variables to test, excluding ID and CREDIT_SCORE itself
predictors_to_test <- setdiff(names(train_data), c("ID", "ANNUAL_MILEAGE"))

# Loop through each predictor and perform the appropriate test
for (var in predictors_to_test) {
  
  # Ignore any columns with no variation
  if (length(unique(train_data[[var]])) < 2) next
  
  # --- Test for association with CONTINUOUS variables ---
  if (is.numeric(train_data[[var]])) {
    # We use a t-test to see if the mean of the numeric variable is
    # different between the 'missing' and 'present' groups.
    test_result <- t.test(train_data[[var]] ~ AM_missing)
    p_value <- test_result$p.value
    cat(sprintf("Variable: %-20s | Test: T-test         | P-value: %.4f\n", var, p_value))
    
  # --- Test for association with CATEGORICAL variables ---
  } else if (is.factor(train_data[[var]]) || is.character(train_data[[var]])) {
    # We use a Chi-squared test for independence between two categorical variables.
    # We add 'simulate.p.value = TRUE' to handle cases with low expected counts.
    test_result <- chisq.test(table(train_data[[var]], AM_missing), simulate.p.value = TRUE)
    p_value <- test_result$p.value
    cat(sprintf("Variable: %-20s | Test: Chi-squared    | P-value: %.4f\n", var, p_value))
  }
}


```
- pvalues very high, so likely that annual_mileage is left empty independently of other variables

- assume from here on that missing values are missing unintentionally



## imputing missing values
```{r}


# 1. Select all numeric columns from train and test sets
train_numeric <- train_data_x %>% select(where(is.numeric))
test_numeric  <- test_data_x %>% select(where(is.numeric))

# 2. Build recipe: Impute ONLY ANNUAL_MILEAGE and CREDIT_SCORE, using all numeric predictors
rec <- recipe(~ ., data = train_numeric) %>%
  step_impute_knn(c("ANNUAL_MILEAGE", "CREDIT_SCORE"))

# 3. Prep the recipe (fit on training data)
rec_prep <- prep(rec, training = train_numeric)

# 4. Impute the training and test sets
train_imputed <- bake(rec_prep, new_data = train_numeric)
test_imputed  <- bake(rec_prep, new_data = test_numeric)

# The result: train_imputed and test_imputed contain all numeric columns,
# with only ANNUAL_MILEAGE and CREDIT_SCORE imputed, using relationships with all numeric variables.

train_data_imputed <- train_data
train_data_imputed$ANNUAL_MILEAGE <- train_imputed$ANNUAL_MILEAGE
train_data_imputed$CREDIT_SCORE <- train_imputed$CREDIT_SCORE

test_data_imputed <- test_data
test_data_imputed$ANNUAL_MILEAGE <- test_imputed$ANNUAL_MILEAGE
test_data_imputed$CREDIT_SCORE <- test_imputed$CREDIT_SCORE

```



- used PCA between numeric variables to impute missing values
- wasnt possible to use the same model to also impute test data
- used knn imputation instead
- doesnt account for categorical variable values!

## histogram of covariates
- supressed as it makes the knitted document untidy
```{r echo = T, results = 'hide'}
for (i in 1:length(data)){
  covariate <- data[[i]]
  if (is.numeric(covariate)){
    hist(covariate, main = paste("Histogram of", names(data)[i] ), xlab = deparse(substitute(covariate)), breaks = 100)
  }
}
```

```{r}
summary(train_data)
```


```{r}

num_cols = train_data_imputed %>% select_if(is.numeric)
cor_matrix = cor(num_cols)
corrplot(cor_matrix, type = "lower", diag = F)
```
- looks like annual mileage has a negative correlation with being married / having children
- ppl with speeding violations are much more likely to have past accidents
- surprisingly non vehicle owners have a higher correlation with making claims


# investigation of claim sizes
```{r echo = T, results = 'hide'}
boxplot(train_data_imputed$CLAIMS ~ train_data_imputed$OUTCOME, 
        main = "Boxplot of claim sizes by outcome", xlab = "OUTCOME", ylab = "CLAIMS")

sum(train_data_imputed[train_data_imputed$OUTCOME == 0, "CLAIMS"])

sum(train_data_imputed[train_data_imputed$CLAIMS == 0, "OUTCOME"])

```
- Looks like there are no cases where outcome = 1 and there is a claimsize of 0
- suggests that claims sizes are always nonzero and zero claims should be accounted for within the counts distribution



## Extreme value analysis of claimsizes
```{r}
nonzerosizes <- data[data$OUTCOME>0, "CLAIMS", drop = T]
quantile(nonzerosizes, c(0.95,0.97, 0.99, 0.995))

# mean excess plot
mrlplot(nonzerosizes, xlim = c(0,45000))
```

- Mean excess function is rising, clearly a heavy tailed distribution

```{r}
# Fitting GEV distribution
block_size <- 50
blocks <- ceiling(seq_along(nonzerosizes) / block_size)
GEVdf <- data.frame(nonzerosizes, blocks)
block_maxima_df <- GEVdf %>%
  group_by(blocks) %>%
  summarise(
    max_claim = max(nonzerosizes) 
  )
gev_fit <- extRemes::fevd(block_maxima_df$max_claim)
summary(gev_fit)
plot(gev_fit)
```


```{r}
# Hypothesis test with H0: shape = 0
# Assuming a normal distribution of the shape parameter estimator
shape.estimate <- 0.219102
shape.stderr <- 0.101009

print(shape.estimate/shape.stderr)
```
- clearly, the shape parameter is significantly different from 0, so we can reject the null hypothesis of an exponential tail. It is quite obvious that we have a frechet type distribution of the nonzero claim sizess

- Suggests use of frechet family
- Could use gamma or lognormal from the exponential family


## lognormal model

```{r}
# lognormal
# lm is equivalent to fitting glm(., gaussian(link = identity))
lognormal_claimsize_df <- train_data_imputed[train_data_imputed$OUTCOME >0,] %>% 
  select(-OUTCOME)
names(train_data_imputed)
lognormal_claimsize_df$CLAIMS <- log(lognormal_claimsize_df$CLAIMS)

# base model no interactions
claimsize.0.lognormal <- lm(CLAIMS ~., 
                   data = lognormal_claimsize_df)


names(train_data_x)
# model with interactions
claimsize.1.lognormal <- lm(CLAIMS ~(AGE + GENDER + DRIVING_EXPERIENCE + EDUCATION +
                             CREDIT_SCORE + VEHICLE_OWNERSHIP + VEHICLE_YEAR +
                             MARRIED + CHILDREN + ANNUAL_MILEAGE +
                             VEHICLE_TYPE + SPEEDING_VIOLATIONS + PAST_ACCIDENTS)^2, 
                   data = lognormal_claimsize_df)
```


```{r}
par(mfrow = c(2,2))
print('---------------------FULL MODEL NO INTERACTION TERMS------------------------')
summary(claimsize.0.lognormal);plot(claimsize.0.lognormal)
print('----------------WHITE TEST-----------------------------')
white_test(claimsize.0.lognormal)
print('-----------------VIF----------------------------')
vif(claimsize.0.lognormal, type = "predictor")
```

- good fit around the middle, but the fit near the tails suffer
- doesnt seem heteroskedastic, linear model is appropriate in that regards
- introducing interaction terms increases adj Rsq
- not much multicollinearity between variables w no interaction terms



### Variable selection
- only investigate adjr2 due to the size of the predictorspace
```{r, eval = FALSE}
# forward selection
# not running because it takes too long to knit
print('----------------------FORWARD----------------------')
lognormal_fwdselection <- ols_step_forward_adj_r2(claimsize.1.lognormal)
lognormal_fwdselection


```

```{r}
# backward selection
print('----------------------backward----------------------')
print('adjr2')
lognormal_bwdselection <- stats::step(claimsize.1.lognormal, direction = "backward", trace = 0)
summary(lognormal_bwdselection)
```
- improvement in adjusted R squared from full model
- backward selection models have more variables but higher adjRsq
- Select out of backward selection models for this

- all seem to have same adj R sq and AIC, choose one with the lowest number of variables, i.e the model chosen by prioritising AIC

```{r, eval = FALSE}
# backward stepwaise
lognormal_fwdselection<- stats::step(claimsize.1.lognormal, direction = "forward", trace = 0)
summary(lognormal_fwdselection)
```
- backward selected model is chosen as the reduced model due to the increased adjusted R squared and parsimony

```{r}
claimsize.2.lognormal <- lognormal_bwdselection
```



### Significance test
```{r}
# between full model with all interaction terms and backward stepwise model
anova(claimsize.2.lognormal,claimsize.1.lognormal)
```
- Clearly, the dropped variables were not that significant






### Residual analysis

```{r}
# plot of type 1 standardised residuals
hist(summary(claimsize.2.lognormal)$res/ summary(claimsize.2.lognormal)$sigma, breaks = 100,
     main = "histogram of standardised residuals", xlab = "standardised residuals")

```
- looks left skewed, suggests havent taken account of patterns in the data, possibly due to poor treatment of tail

## Gamma model

- consider common interaction terms and interaction terms suggested by correlation analysis
```{r}
claimsize_df <- train_data_imputed[train_data_imputed$OUTCOME >0,] %>%
  select(-OUTCOME)
```


```{r}
# base model no interaction
claimsize.0.gamma <- glm(CLAIMS ~. , family = Gamma(link = "log"),
                   data = claimsize_df)

# w interactions
claimsize.1.gamma <- glm(CLAIMS ~(AGE + GENDER + DRIVING_EXPERIENCE + EDUCATION +
                             CREDIT_SCORE + VEHICLE_OWNERSHIP + VEHICLE_YEAR +
                             MARRIED + CHILDREN + ANNUAL_MILEAGE +
                             VEHICLE_TYPE + SPEEDING_VIOLATIONS + PAST_ACCIDENTS)^2, family = Gamma(link = "log"),
                   data = claimsize_df)
par(mfrow = c(2,2))
print('---------------------FULL MODEL NO INTERACTION TERMS------------------------')
summary(claimsize.0.gamma);plot(claimsize.0.gamma)
print('----------------WHITE TEST-----------------------------')
white_test(claimsize.0.gamma)
print('-----------------VIF----------------------------')
vif(claimsize.0.gamma, type = "predictor")
```



### Variable selection

```{r echo = T, results = 'hide'}
df <- claimsize_df


# Ensure positivity
stopifnot(all(df$CLAIMS > 0), all(is.finite(df$CLAIMS)))

# Make sure all categorical vars are factors, then drop unused levels
char_cols <- sapply(df, is.character)
df[char_cols] <- lapply(df[char_cols], factor)
df <- droplevels(df)

upper_form <- CLAIMS ~ (AGE + GENDER + DRIVING_EXPERIENCE + EDUCATION +
                        CREDIT_SCORE + VEHICLE_OWNERSHIP + VEHICLE_YEAR +
                        MARRIED + CHILDREN + ANNUAL_MILEAGE +
                        VEHICLE_TYPE + SPEEDING_VIOLATIONS + PAST_ACCIDENTS)^2

# Start with a converged base model (intercept + main effects is a good start)
base_form <- CLAIMS ~ AGE + GENDER + DRIVING_EXPERIENCE + EDUCATION +
                      CREDIT_SCORE + VEHICLE_OWNERSHIP + VEHICLE_YEAR +
                      MARRIED + CHILDREN + ANNUAL_MILEAGE +
                      VEHICLE_TYPE + SPEEDING_VIOLATIONS + PAST_ACCIDENTS

gamma_base <- glm(base_form, family = Gamma(link = "log"),
                  data = df, control = glm.control(maxit = 100))

# Forward or both-direction stepwise within scope; pass control so refits get more iterations
gamma_step <- stats::step(gamma_base,
                   scope = list(lower = ~1, upper = upper_form),
                   direction = "both",
                   trace = 0,
                   control = glm.control(maxit = 70))
summary(gamma_step)
```


```{r echo = T, results = 'hide'}


cat("\n\n--- Generating Diagnostic Plots for Final Model ---\n")
claimsize.2.gamma <- gamma_step

par(mfrow = c(2, 2))
plot(claimsize.2.gamma)
exp(coef(claimsize.2.gamma))


print("--------------------------------")
vif(claimsize.2.gamma)

```
- high vif because we have interaction terms now
- we opt to keep CHILDREN despite the high vif due to the pirnciple of hierarchy, to ensure model interpretation as otherwise interpretation becomes difficult


#### likelihood ratio tests
```{r}
# between full model and stepwise selected
anova(claimsize.2.gamma, claimsize.1.gamma, test = "LRT")
```
- removal of the interaction terms doesnt seem significant under H0, meaning the stepwise selected model is preferred.
 

### Residual analysis

```{r}
# histogram of type 1 standardised residuals
hist(rstandard(claimsize.2.gamma), breaks = 100)
```
- approximately centered around mean of zero, but still some skewness


- signs of heteroskedasticity as residuals are not uniform over fitted values

### identifying influential points (outliers, high leverage)

```{r echo = T, results = 'hide'}

model_diagnostics <- augment(claimsize.2.gamma,
                             data = claimsize_df)
# Calculate the Cook's distance threshold
n <- nrow(claimsize_df)
cooks_threshold <- 4 / n
print(paste('cooks threshold is ', cooks_threshold))
# Find the observations that exceed this threshold
influential_points <- model_diagnostics %>%
  filter(.cooksd > cooks_threshold) %>%
  arrange(desc(.cooksd))
print(influential_points)
```

```{r echo = T, results = 'hide'}
# investigating influential points further
# correlation between variables when influential 
num_cols <- influential_points %>%
  dplyr::select(where(is.numeric))
corrplot(cor(num_cols), type = "lower", diag = F,tl.cex = 0.7)


#histogram
for (i in 1:length(influential_points)){
  covariate <- influential_points[[i]]
  if (is.numeric(covariate)){
    hist(covariate, main = paste("Histogram of", names(influential_points)[i] ), xlab = deparse(substitute(covariate)), breaks = 80)
  }
}

```

- There seems to be one case with an extremely high claim size (approx 6x the next biggest), which is likely to be a major influential point 

```{r}
# plotting influential points
ggplot(model_diagnostics, aes(x = .hat, y = .std.resid)) +
  # Points are sized by their Cook's distance
  geom_point(aes(size = .cooksd), alpha = 0.5, shape = 1) +
  
  # Add a smoother to see the general trend
  geom_smooth(se = FALSE, col = "dodgerblue") +
  
  # Highlight the most influential points found earlier
  geom_point(data = influential_points, aes(size = .cooksd), color = "red") +
  
  # Add labels to the influential points (e.g., by row number)
  geom_text(data = influential_points, aes(label = rownames(influential_points)),
            vjust = -1, color = "red") +
  
  # Add a horizontal line at 0
  geom_hline(yintercept = 0, linetype = "dashed", color = "gray50") +
  
  # Add labels and a title
  labs(
    title = "Influence Plot: Residuals vs. Leverage",
    subtitle = "Points sized by Cook's Distance. Red points are highly influential.",
    x = "Leverage (Hat Values)",
    y = "Standardized Deviance Residuals",
    size = "Cook's D"
  ) +
  theme_minimal()
```
- opted for cooks distance as it combines the outlier and high leverage status of points into one metric
- 96 influential points

#### investigating how influential points affect our model
```{r}
influential_rows <- as.numeric(rownames(influential_points))
claimsize_df_no_influencers <- claimsize_df[-influential_rows, ]
model_without_influencers <- glm(formula(claimsize.2.gamma),
                                 family = Gamma(link = "log"),
                                 data = claimsize_df_no_influencers)


comparison <- cbind(Original = coef(claimsize.2.gamma),
                    No_Influencers = coef(model_without_influencers))

print(comparison)

c(Original_Dispersion = summary(claimsize.2.gamma)$dispersion, No_Influencers_Dispersion = summary(model_without_influencers)$dispersion)

```
- not much difference in dispersion
- influential points arent doing too much to significantly affect the model fit





## Testing the Gamma and Lognormal model
```{r}
# Helper: coerce factor levels in newdata to training levels stored in the fit
prep_newdata_for <- function(newdata, fit) {
  nd <- newdata
  if (!is.null(fit$xlevels)) {
    for (nm in names(fit$xlevels)) {
      if (nm %in% names(nd)) {
        nd[[nm]] <- factor(nd[[nm]], levels = fit$xlevels[[nm]])
      }
    }
  }
  droplevels(nd)
}

# 1) Test set for severity evaluation (only positive claims)
test_claimsize_df <- test_data_imputed |>
  dplyr::filter(CLAIMS > 0)



# 2) Align test data to each model
test_for_gamma    <- prep_newdata_for(test_claimsize_df, claimsize.2.gamma)
test_for_lognorm  <- prep_newdata_for(test_claimsize_df, claimsize.2.lognormal)

# 3) Predictions
# Gamma(log): already on the dollar scale
pred_gamma <- predict(claimsize.2.gamma, newdata = test_for_gamma, type = "response")

# Lognormal (lm on log(CLAIMS)): use smearing correction
pred_log <- predict(claimsize.2.lognormal, newdata = test_for_lognorm)  # predicts log(CLAIMS)

# Duan’s smearing factor from TRAINING residuals of the log model
sf <- mean(exp(residuals(claimsize.2.lognormal)), na.rm = TRUE)
pred_lognormal <- exp(pred_log) * sf


# 4) Metrics
actual_claims <- test_for_lognorm$CLAIMS

rmse <- function(a, p) sqrt(mean((a - p)^2, na.rm = TRUE))
mae  <- function(a, p) mean(abs(a - p), na.rm = TRUE)

rmse_gamma <- rmse(actual_claims, pred_gamma)
mae_gamma  <- mae(actual_claims, pred_gamma)

rmse_lognormal <- rmse(actual_claims, pred_lognormal)
mae_lognormal  <- mae(actual_claims, pred_lognormal)

comparison_df <- data.frame(
  Model = c("Gamma GLM", "Lognormal (lm + smearing)"),
  RMSE  = c(rmse_gamma, rmse_lognormal),
  MAE   = c(mae_gamma,  mae_lognormal)
)
print(comparison_df)
```


When we fit a lognormal severity model (lm on log(CLAIMS)), the model predicts the mean of the log outcome. Simply exponentiating those predictions gives the conditional median on the dollar scale and underestimates the mean (Jensen’s inequality). To obtain unbiased mean severities, we apply a back‑transformation correction: Duan’s smearing factor (multiply by the average of exp(residuals)) or, under homoskedastic normal log‑errors, multiply by exp(sigma2/2). Gamma GLM predictions (log link) already return the mean and need no correction.


```{r}
# plot errors

hist(pred_gamma - actual_claims, breaks = 100, xlim = c(-80000, 40000),
     main = 'gamma model errors')

hist(pred_lognormal - actual_claims, breaks = 100, xlim = c(-80000, 40000),
     main = 'lognormal model errors')
```
- There are some extreme tail cases that our models mis, leading to a skewed error distribution

```{r}
# computing average errors
print(paste("gamma model", mean(pred_gamma - actual_claims)))
print(paste("lognormal model", mean(pred_lognormal - actual_claims)))
```

```{r}
# observing tail behaviours of the errors
gamma_model_errors <- pred_gamma - actual_claims
gamma_model_errors_tail <- abs(gamma_model_errors[gamma_model_errors  < (-10)])
lognormal_model_errors <- pred_lognormal - actual_claims
lognormal_model_errors_tail <- abs(lognormal_model_errors[lognormal_model_errors  < (-10)])

```

```{r}

# --- 1. Define Your Data and Names ---
# Replace these placeholder vectors with your actual data
vector1 <- gamma_model_errors_tail # e.g., errors_gamma
vector2 <- lognormal_model_errors_tail   # e.g., errors_lognormal

# Define names for the legend
name1 <- "Gamma Model Errors"
name2 <- "Lognormal Model Errors"




# --- Plot 1: Log-Log Survival Plot ---

ecdf_x <- ecdf(vector1)
data_x <- data.frame(val = sort(unique(vector1))) %>% mutate(prob = 1 - ecdf_x(val)) %>% filter(prob > 0)
ecdf_y <- ecdf(vector2)
data_y <- data.frame(val = sort(unique(vector2))) %>% mutate(prob = 1 - ecdf_y(val)) %>% filter(prob > 0)
xlim_range <- range(c(data_x$val, data_y$val))
ylim_range <- range(c(data_x$prob, data_y$prob))
plot(data_x$val, data_x$prob, type = "p", log = "xy", col = "dodgerblue", lwd = 2,
     xlim = xlim_range, ylim = ylim_range, main = "Log-Log Survival Plot",
     xlab = "Value (log scale)", ylab = "P(Value > x) (log scale)")
points(data_y$val, data_y$prob, col = "firebrick", lwd = 2, lty = 2)
legend("bottomleft", legend = c(name1, name2), col = c("dodgerblue", "firebrick"), lty = c(1, 2), lwd = 2, bty = "n")

# --- Plot 2: Mean Excess Plot ---

mean_excess_func <- function(data) {
  thresholds <- unique(sort(data))
  excess <- sapply(thresholds, function(u) { mean(data[data > u] - u) })
  return(data.frame(threshold = thresholds, mean_excess = excess))
}
me_x <- mean_excess_func(vector1)
me_y <- mean_excess_func(vector2)
xlim_range_me <- range(c(me_x$threshold, me_y$threshold))
ylim_range_me <- range(c(me_x$mean_excess, me_y$mean_excess), na.rm = TRUE)
plot(me_x$threshold, me_x$mean_excess, type = "l", col = "dodgerblue", lwd = 2,
     xlim = c(0, 20000), ylim = ylim_range_me, main = "Mean Excess Plot",
     xlab = "Threshold (u)", ylab = "Mean Excess over Threshold e(u)")
lines(me_y$threshold, me_y$mean_excess, col = "firebrick", lwd = 2, lty = 2)
legend("topleft", legend = c(name1, name2), col = c("dodgerblue", "firebrick"), lty = c(1, 2), lwd = 2, bty = "n")


```

```{r}
# AIC for the Gamma(GLM) model (already on Y-scale)
aic_gamma <- AIC(claimsize.2.gamma)

# AIC for the lognormal model from lm(log(CLAIMS) ~ ...)
# Convert the lm to a lognormal likelihood on Y
mf_ln  <- model.frame(claimsize.2.lognormal)
y_log  <- model.response(mf_ln)               # = log(CLAIMS)
y      <- exp(y_log)                          # original scale
mu_log <- fitted(claimsize.2.lognormal)       # mean on log scale
sdlog  <- sigma(claimsize.2.lognormal)        # residual SD on log scale

ll_lognorm <- sum(dlnorm(y, meanlog = mu_log, sdlog = sdlog, log = TRUE))
k_lognorm  <- length(coef(claimsize.2.lognormal)) + 1  # +1 for sd
aic_lognorm <- -2 * ll_lognorm + 2 * k_lognorm

# (Equivalent shortcut: aic_lognorm <- AIC(claimsize.2.lognormal) + 2 * sum(log(y)))

# Compare
aics <- c(Gamma = aic_gamma, Lognormal = aic_lognorm)
delta <- aics - min(aics)
akaike_wt <- exp(-0.5 * delta) / sum(exp(-0.5 * delta))

data.frame(Model = names(aics), AIC = aics, DeltaAIC = delta, AkaikeWeight = akaike_wt)
```



```{r}
# QQ Plot against Exponential (for tail diagnosis)
qqtail <- function(vec, col, add=FALSE, ...) {
  n <- length(vec)
  sorted <- sort(vec)
  k <- floor(0.1 * n)
  tail <- sorted[(n-k+1):n]
  qexp <- qexp(ppoints(k), rate=1/mean(tail - min(tail)))
  if (!add) {
    plot(qexp, tail, col=col, pch=16, xlab="Theoretical Quantiles (Exp)", 
         ylab="Sample Quantiles", main="QQ Plot (Tail)", ...)
  } else {
    points(qexp, tail, col=col, pch=16)
  }
}

qqtail(gamma_model_errors_tail, "blue")
qqtail(lognormal_model_errors_tail, "red", add=TRUE)
legend("topleft", legend=c("Gamma errors", "Lognormal errors"), col=c("blue", "red"), pch=16)

# Hill Plot (Pareto tail index estimator)
hillplot <- function(vec, col, add=FALSE, ...) {
  sorted <- sort(vec, decreasing=TRUE)
  n <- length(vec)
  k <- 2:(n-1)
  hill <- sapply(k, function(i) mean(log(sorted[1:i])) - log(sorted[i]))
  if (!add) {
    plot(k, hill, type="l", col=col, lwd=2,
         xlab="Order Statistics k", ylab="Hill Estimator", main="Hill Plot", ...)
  } else {
    lines(k, hill, col=col, lwd=2)
  }
}

hillplot(gamma_model_errors_tail, "blue")
hillplot(lognormal_model_errors_tail, "red", add=TRUE)
legend("topright", legend=c("Gamma errors", "Lognormal errors"), col=c("blue", "red"), lwd=2)

# Maximum-to-Sum Plot (finite moments check)
max2sumplot <- function(vec, col, add=FALSE, ...) {
  n <- length(vec)
  k <- 1:n
  maxvals <- sapply(k, function(i) max(vec[1:i]))
  sumvals <- sapply(k, function(i) sum(vec[1:i]))
  ratio <- maxvals/sumvals
  if (!add) {
    plot(k, ratio, type="l", col=col, lwd=2,
         xlab="Sample Size", ylab="Max/Sum", main="Maximum-to-Sum Plot", ...)
  } else {
    lines(k, ratio, col=col, lwd=2)
  }
}

max2sumplot(gamma_model_errors_tail, "blue")
max2sumplot(lognormal_model_errors_tail, "red", add=TRUE)
legend("topright", legend=c("Gamma errors", "Lognormal errors"), col=c("blue", "red"), lwd=2)
```

- Not much difference in the tail, however we choose Gamma model largely due to the difference in AIC.

- Theory says lognormal has a heavier tail than gamma; diagnostics are consistent but the difference is small in this dataset.
- Both models exhibit very similar tail behaviour on residuals and near‑identical predictive accuracy.
- the gamma model is a reasonable, slightly better‑scoring choice.






# Investigation of claim outcome
```{r}
claimoutcome_df <- train_data_imputed %>%select(-CLAIMS)
mean(claimoutcome_df$OUTCOME)
```

## logistic regression
```{r}
# full basic model without interaction terms
outcome.0.logistic <- glm(OUTCOME ~ .,
                              family = binomial(link = "logit"),
                              data = claimoutcome_df)

# full model with interaction terms
outcome.1.logistic <- glm(OUTCOME ~(AGE + GENDER + DRIVING_EXPERIENCE + EDUCATION +
                             CREDIT_SCORE + VEHICLE_OWNERSHIP + VEHICLE_YEAR +
                             MARRIED + CHILDREN + ANNUAL_MILEAGE +
                             VEHICLE_TYPE + SPEEDING_VIOLATIONS + PAST_ACCIDENTS)^2,
                          family= binomial(link = "logit"),
                          data = claimoutcome_df)
```

### Variable selection

Stepwise selection based on AIC
```{r echo = T, results = 'hide'}
# Stepwise (both) from null -> up to the full formula of outcome.1.logistic
logistic_both <- stats::step(
  update(outcome.1.logistic, . ~ 1),                       # start: intercept-only (same data/family)
  scope = list(lower = ~ 1, upper = formula(outcome.1.logistic)),
  direction = "both",
  trace = 0,
  k = log(nobs(outcome.1.logistic))                         # BIC; use k = 2 for AIC
)

summary(logistic_both)

# Final chosen model
outcome.2.logistic <- logistic_both
```



### hypothesis tests
```{r}
# Align both models to the exact same rows used in the full model
mf <- model.frame(outcome.1.logistic)
reduced <- update(outcome.2.logistic, data = mf)

# Likelihood-ratio test (valid when reduced is nested in full)
anova(reduced, outcome.1.logistic, test = "Chisq")

# AIC and BIC (smaller is better)
AIC(reduced, outcome.1.logistic)

n <- nobs(outcome.1.logistic)
AIC(reduced, outcome.1.logistic, k = log(n))  # BIC
```
- The LR test suggests some in-sample gain from the full model, but parsimony (AIC/BIC) and predictive validation likely favor the reduced model unless you see a clear lift on a hold-out set.


### Residual analysis and finding influential points
```{r}
hist(residuals(outcome.2.logistic, type = "deviance"), main = "Histogram of Deviance Residuals", xlab = "Deviance Residuals", breaks = 100)
```
- we see a non normal shape as the response variable is discrete and has a high number of 0's
- look at randomised quantile residuals instead
```{r}
qres <- qresiduals(outcome.2.logistic)
par(mfrow = c(1, 2)) # Set up a 1x2 plotting area
hist(qres, main = "Histogram of Quantile Residuals", xlab = "Quantile Residuals")
qqnorm(qres, main = "Normal Q-Q Plot")
qqline(qres, col = "red", lty = 2)
```
- We see that the quantile residuals are normally distributed, and the qq plot shows a very close fit to the normal distribution, suggesting that our logistic regression model is appropriate for the data.

## Probit regression
- using binomial with probit link

```{r}
# full basic model without interaction terms
outcome.0.probit <- glm(OUTCOME ~ .,
                        family = binomial(link = "probit"),
                        data = claimoutcome_df)

# full model with interaction terms
outcome.1.probit <- glm(OUTCOME ~(AGE + GENDER + DRIVING_EXPERIENCE + EDUCATION +
                                  CREDIT_SCORE + VEHICLE_OWNERSHIP + VEHICLE_YEAR +
                                  MARRIED + CHILDREN + ANNUAL_MILEAGE +
                                  VEHICLE_TYPE + SPEEDING_VIOLATIONS + PAST_ACCIDENTS)^2,
                        family = binomial(link = "probit"),
                        data = claimoutcome_df)
```

### Variable selection

```{r echo = T, results = 'hide'}
probit_both <- stats::step(
  update(outcome.1.probit, . ~ 1),  # start from intercept-only
  scope = list(lower = ~ 1, upper = formula(outcome.1.probit)),
  direction = "both",
  trace = 0,
  k = log(nobs(outcome.1.probit))   # BIC; use k = 2 for AIC
)

summary(probit_both)

# Final chosen model
outcome.2.probit <- probit_both
```

### hypothesis tests

```{r}
# Ensure both models use identical rows
mf <- model.frame(outcome.1.probit)
reduced <- update(outcome.2.probit, data = mf)

# Likelihood-ratio test (Model 1 = reduced, Model 2 = full)
anova(reduced, outcome.1.probit, test = "Chisq")

# AIC and BIC comparison (smaller is better)
AIC(reduced, outcome.1.probit)

n <- nobs(outcome.1.probit)
AIC(reduced, outcome.1.probit, k = log(n))  # BIC
```


## Comparison between logit and probit link functions

```{r}
# Helpers
prep_newdata_for <- function(newdata, fit) {
  nd <- newdata
  if (!is.null(fit$xlevels)) {
    for (nm in names(fit$xlevels)) if (nm %in% names(nd)) {
      nd[[nm]] <- factor(nd[[nm]], levels = fit$xlevels[[nm]])
    }
  }
  droplevels(nd)
}
log_loss <- function(y, p, eps = 1e-15) {
  p <- pmin(pmax(p, eps), 1 - eps)
  -mean(y * log(p) + (1 - y) * log(1 - p), na.rm = TRUE)
}
brier <- function(y, p) mean((p - y)^2, na.rm = TRUE)
calibration_df <- function(y, p, bins = 10, label = "model") {
  brks <- quantile(p, probs = seq(0, 1, length.out = bins + 1), na.rm = TRUE)
  g <- cut(p, breaks = unique(brks), include.lowest = TRUE)
  dplyr::summarise(dplyr::group_by(data.frame(y, p, g), g),
                   mean_p = mean(p, na.rm = TRUE),
                   obs   = mean(y, na.rm = TRUE),
                   n     = dplyr::n()) |>
    mutate(model = label)
}

# Prepare test data for each model (handles factors/levels)
test_logit  <- prep_newdata_for(test_data_imputed, outcome.2.logistic)
test_probit <- prep_newdata_for(test_data_imputed, outcome.2.probit)

# Predictions
y  <- test_logit$OUTCOME
p_logit  <- predict(outcome.2.logistic, newdata = test_logit,  type = "response")
p_probit <- predict(outcome.2.probit,   newdata = test_probit, type = "response")

# Keep rows where both models produce probabilities
keep <- is.finite(p_logit) & is.finite(p_probit) & !is.na(y)
y <- y[keep]; p_logit <- p_logit[keep]; p_probit <- p_probit[keep]

# Build ROC objects with pROC
roc_logit  <- pROC::roc(y, p_logit,  quiet = TRUE)
roc_probit <- pROC::roc(y, p_probit, quiet = TRUE)

# Get numeric AUCs (either of these styles works)
auc_logit  <- as.numeric(pROC::auc(roc_logit))
auc_probit <- as.numeric(pROC::auc(roc_probit))
# or:
# auc_logit  <- as.numeric(roc_logit$auc)
# auc_probit <- as.numeric(roc_probit$auc)

metrics <- data.frame(
  Model   = c("Logit", "Probit"),
  AUC     = c(auc_logit, auc_probit),
  LogLoss = c(log_loss(y, p_logit),  log_loss(y, p_probit)),
  Brier   = c(brier(y, p_logit),     brier(y, p_probit))
)
print(metrics, row.names = FALSE)

# AUC difference test
auc_test <- pROC::roc.test(roc_logit, roc_probit, method = "delong")
cat(sprintf("\nDeLong test for AUC difference: z = %.3f, p = %.4f\n",
            auc_test$statistic, auc_test$p.value))

# ROC curves (overlaid)
df_roc_logit <- data.frame(
  fpr = rev(1 - roc_logit$specificities),
  tpr = rev(roc_logit$sensitivities),
  model = "Logit"
)
df_roc_probit <- data.frame(
  fpr = rev(1 - roc_probit$specificities),
  tpr = rev(roc_probit$sensitivities),
  model = "Probit"
)
df_roc <- rbind(df_roc_logit, df_roc_probit)

p_roc <- ggplot(df_roc, aes(x = fpr, y = tpr, color = model)) +
  geom_line(size = 1) +
  geom_abline(slope = 1, intercept = 0, linetype = 2, color = "gray50") +
  labs(title = sprintf("ROC on test (AUC: logit=%.3f, probit=%.3f)",
                       metrics$AUC[metrics$Model=="Logit"],
                       metrics$AUC[metrics$Model=="Probit"]),
       x = "False Positive Rate", y = "True Positive Rate") +
  theme_minimal() + coord_equal()
print(p_roc)

# Calibration (reliability) plot
cal_logit  <- calibration_df(y, p_logit,  bins = 10, label = "Logit")
cal_probit <- calibration_df(y, p_probit, bins = 10, label = "Probit")
cal <- rbind(cal_logit, cal_probit)

p_cal <- ggplot(cal, aes(x = mean_p, y = obs, color = model, size = n)) +
  geom_point(alpha = 0.85) +
  geom_abline(slope = 1, intercept = 0, linetype = 2, color = "gray50") +
  scale_size_continuous(name = "Bin N") +
  labs(title = "Calibration on test (10 quantile bins)",
       x = "Mean predicted probability", y = "Observed frequency") +
  theme_minimal()
print(p_cal)

# Optional: density of predicted probabilities by class (separation plot)
df_prob <- data.frame(y = factor(y, levels = c(0,1)),
                      logit = p_logit, probit = p_probit) |>
  tidyr::pivot_longer(cols = c(logit, probit), names_to = "model", values_to = "p")
p_den <- ggplot(df_prob, aes(x = p, fill = y)) +
  geom_density(alpha = 0.35) +
  facet_wrap(~ model, ncol = 1) +
  labs(title = "Predicted probability densities by outcome (test)",
       x = "Predicted probability", y = "Density", fill = "Outcome") +
  theme_minimal()
print(p_den)
```

- Predicted‑probability densities by outcome
Clear separation: class 0 probabilities are heavily concentrated near 0; class 1 probabilities are mostly 0.5–1.0 with a peak near 0.8–0.95.
Overlap is mainly in 0.1–0.3, which is where most errors will occur.
Logit vs probit look almost identical; probit shows a hair more mass near very high probabilities, but the difference is tiny.


- Calibration (10 quantile bins)
Points lie very close to the 45° line for both models across the range → well‑calibrated probabilities.
Minor, likely noise‑level deviations: around 0.7–0.8 the probit is slightly underconfident (observed > predicted), around ~0.55 the logit is slightly overconfident. Bin sizes are ~200, so sampling variation can explain this.


- ROC and AUC
Curves overlap almost perfectly; test AUCs are 0.881 (logit) vs 0.882 (probit).
The 0.001 gap is negligible and would not be statistically or practically significant (DeLong test would almost surely be non‑significant).
Bottom line

- On the test set, logit and probit are essentially indistinguishable
- Choose Logit due to its superior explanatory power

# Final model
- Binomial with logit link for claim occurrence
- Gamma GLM with log link for claim severity
```{r}
# kNN-impute numeric predictors and refit models on data_imputed

# Predictors used by either model (exclude responses)
pred_vars <- setdiff(
  unique(c(all.vars(formula(outcome.2.logistic))[-1],
           all.vars(formula(claimsize.2.gamma))[-1])),
  c("OUTCOME", "CLAIMS")
)

# Numeric predictors to impute
num_vars <- intersect(pred_vars, names(Filter(is.numeric, data)))

# Train kNN imputer (k = 5) on training numerics and impute
rec <- recipe(~ ., data = data[, num_vars, drop = FALSE]) |>
  step_impute_knn(all_predictors(), neighbors = 5)
imp <- prep(rec, training = data[, num_vars, drop = FALSE], retain = TRUE)

data_imputed <- data
data_imputed[, num_vars] <- bake(imp, new_data = data[, num_vars, drop = FALSE])


# Refit models with identical specs on imputed data
claimsize.3.gamma  <- update(claimsize.2.gamma,  data = data_imputed[data_imputed$CLAIMS > 0, ])
outcome.3.logistic <- update(outcome.2.logistic, data = data_imputed)
```

## summaries of final models
```{r}
summary(claimsize.3.gamma)
exp(coef(claimsize.3.gamma))  # multiplicative effects on mean CLAIMS

```

```{r}
summary(outcome.3.logistic)
```







```{r}
dir.create("models", showWarnings = FALSE)
dir.create("data",   showWarnings = FALSE)

saveRDS(outcome.3.logistic, "models/outcome_3_logistic.rds")
saveRDS(claimsize.3.gamma,   "models/claimsize_3_gamma.rds")
saveRDS(data,                 "data/training_data.rds")
```

```{r}
cat("Saving required objects to 'app_dependencies.RData'...\n")

save(data,
     outcome.3.logistic,
     claimsize.3.gamma,
     data,
     file = "app_dependencies.RData")

cat("Objects saved successfully.\n")

```




